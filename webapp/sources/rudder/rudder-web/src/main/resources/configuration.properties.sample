#####################################################################################
# Copyright 2011 Normation SAS
#####################################################################################
#
# This file is part of Rudder.
#
# Rudder is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# In accordance with the terms of section 7 (7. Additional Terms.) of
# the GNU General Public License version 3, the copyright holders add
# the following Additional permissions:
# Notwithstanding to the terms of section 5 (5. Conveying Modified Source
# Versions) and 6 (6. Conveying Non-Source Forms.) of the GNU General
# Public License version 3, when you create a Related Module, this
# Related Module is not considered as a part of the work and may be
# distributed under the license agreement of your choice.
# A "Related Module" means a set of sources files including their
# documentation that, without modification of the Source Code, enables
# supplementary functions or services in addition to those offered by
# the Software.
#
# Rudder is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Rudder.  If not, see <http://www.gnu.org/licenses/>.

#
#####################################################################################

##
# Default configuration file for the application.
# You can define the location of the file by
# setting "rudder.configFile" JVM property,
# for example:
# java .... -Drudder.configFile=/opt/rudder/etc/rudder-web.conf
##


##########################
# Application information ###########################################################
##########################

#
# Directory used to store locks about
# some Rudder actions or batch processing
#
rudder.dir.lock=/var/rudder/lock/

#
# Location of the relay api used by rudder webapp
# It's the base url of relay api, Rudder will manage to call the correct url from that base
#
rudder.server.relay.api=https://localhost/rudder/relay-api

#
# Add HSTS header to enforce HTTPS for Rudder Web interface.
# It is set for 1 year. and does not include the subdomains by default.
#
# You MUST NOT enable it if you are using plain HTTP for other sites or applications
# on the same domain as your Rudder server.
#
rudder.server.hsts=false

#
# Enable the subdomains option in the HSTS header.
#
# You MUST NOT enable it if you rely on plain HTTP for other sites or applications
# on any (present or future) subdomain of you Rudder server's domain.
#
rudder.server.hstsIncludeSubDomains=false

##################
# LDAP properties ###################################################################
##################

#
# LDAP directory connection information
#
ldap.host=localhost
ldap.port=389
ldap.authdn=cn=manager,cn=rudder-configuration
ldap.maxPoolSize=2

#
# Password used to connect to the OpenLDAP server.
# On a standard Rudder installation, the password is managed in
# "/opt/rudder/etc/rudder-passwords.conf" and the value below
# will be overridden.
#
ldap.authpw=secret

###########################
# SQL database properties  ##########################################################
###########################

#
# Only PostgreSQL is supported, and so the expected
# JDBC driver is "org.postgresql.Driver". You should not change that.
#
rudder.jdbc.driver=org.postgresql.Driver

#
# Connection URL to your postgresql server. On a default Rudder installation,
# the database is located on the root server and Rudder managed it: it will ensure
# that passwords is well configured, that the service is up and running and restarted
# when needed, etc. If you want to use that option, don't change URL below.
# If you want to use a dedicated remote server for PostgreSQL, set its connection
# URL here, and set parameter `rudder.postgresql.local` to false
#
rudder.jdbc.url=jdbc:postgresql://localhost:5432/rudder
rudder.postgresql.local=true

#
# Login and password used to connect to the PostgreSQL server.
# On a standard Rudder installation, the password is managed in
# "/opt/rudder/etc/rudder-passwords.conf" and the value below
# will be overridden. In case of authentication problem, check that
# the passwords matches.
#
rudder.jdbc.username=rudder
rudder.jdbc.password=Normation

#
# Connection pool size. Rudder does not do a lot of connection in parallel,
# so you should not have to change that value.
#
rudder.jdbc.maxPoolSize=25

#
# Batch size for memory or database intensive request.
# This will split big query in smaller queries. You can try to make it bigger
# to reduce the number of queries and the your PostgreSQL server is up to the task. Check
# for timeout or other SQL related errors in /var/log/rudder/webapp logs.
#
rudder.jdbc.batch.max.size=500

#############################
# Automatic reports cleaning ###########################################################
#############################

#
# Automatic reports cleaning
# This allows you to schedule automatically clean reports (archive or delete)
#

#
# TTL are the maximum age (in days) of reports before archiving (archive.TTL)
# or deleting (delete.TTL)
#
# A value equal to 0 or lower means disable automatic archiving/deleting.
#
# If the archive TTL is greater than the delete TTL then archiving will be disabled
# as there will be no reports to archive (they would be deleted first).
#
# Reports need between 500 and 900 kB per Directive per Node per Day, while
# archived reports need 150 kB per Directive per Node per Day (but archived data
# are not available in the web interface, they are just here for auditability)
#
# Defaults: archive disabled, delete after 4 days.
#

rudder.batch.reportscleaner.archive.TTL=0
rudder.batch.reportscleaner.delete.TTL=4

# Report with level "log" (log_info, log_warn, etc) are not kept more than a couple of runs.
# They are used for debugging purpose, and so have a limited interest span (but can consume a lot
# of DB space).
# The TTL is given as a number of run to keep (the max run period of all nodes will be used),
# with the format: Nx (with N a positive integer); or by a number of minutes: N (with N a positive integer).
# The value is only updated date when rudder starts.
rudder.batch.reportsCleaner.deleteLogReport.TTL=2x

#
# Automatic compliance levels cleaning.
# This allows you to define the period of time during which
# compliance level data for nodes, by run, up to directive
# granularity are kept.
# The tables grow at ~150kB / node / directive / day
# (ie, for 100 nodes, with 15 rules having each of them 10 directives,
# you need ~65GB for a month back of data).
# There is no archive state for compliance levels.
#
rudder.batch.reportscleaner.compliancelevels.delete.TTL=8

#
# Schedule option for automatic cleaning
# Automatic cleaning can be scheduled:
#  - every hour at the minute past the hour of your choice
#  - every day at the time of your choice
#  - every week on the day at the time of your choice
#
# Available options: hourly, daily, weekly
# Default frequency: daily
rudder.batch.reportscleaner.frequency=daily

# Defaults: minute=0, hour=0, day=Sunday


# Which minute the cleaner should be run on.
# Values  : [0-59]
# Default : 0
rudder.batch.databasecleaner.runtime.minute=0

# Which hour the cleaner should be run on.
# Values : [0-23]
# Default : 0
rudder.batch.databasecleaner.runtime.hour=0

# Which day the cleaner should be run on.
# Values : monday | tuesday | wednesday | thursday | friday | saturday | sunday
# Default : sunday
rudder.batch.databasecleaner.runtime.day=sunday

#########################
# Inventories processing ###########################################################
#########################

#
# Inventories are processed through a file watcher
# (inotify) which react to new
# inventories put in ${inventories.root.directory}/incoming.
# You can [start, stop, restart] the watcher with POST to API
# /api/latest/inventories/watcher/[start, stop, restart].
#
inventories.root.directory=/var/rudder/inventories

#
# Period for a batch to run and see if any inventories were
# missed by inotify watchers.
#
inventories.watcher.period.garbage.old=5 minutes

#
# Maximum relative age for inventories before being deleted without
# trying to add them. This can be important if Rudder was down for
# a long time and inventories accumulated, no need to keep the older.
#
inventories.watcher.max.age.before.deletion=3 days

#
# You may want to limit the number of inventory files parsed and save in parallel.
# You can specify a positive integer or a string formatted "Nx" where
# "N" is an Int and x means "number of available core".
# By default, inventories are parsed and saved one with parallelization of 2, which allows
# to keep a lowish LDAP write contention but still avoid a starvation in case one inventory is
# extremely long to parse. On a big server with lots of inventories coming in parallel,
# you can try to rise that number. In case with number of nodes < 500, you can safely set it to 1
# which will make logs simpler to understand.
inventory.parse.parallelization=2

#
# In some case, <PROCESSES> cause performance problems in rudder web-app
# and they are not used. You can ignore them with that property set to "true".
# In that case, node's processes will appear to be empty in node details.
# Boolean, default to "false" (ie: processes are parsed and displayed)
inventory.parse.ignore.processes=false

#
# You can keep exhaustive information about LDAP base modification
# happening in relation to inventory processing. It is mostly used
# for debug.
# You can enable that log by setting the following logger in
# /opt/rudder/etc/logback.xml file to "trace" level (default "off"):
# <logger name="trace.ldif.in.file" level="trace" />
#
ldif.tracelog.rootdir=/var/rudder/inventories/debug

#
# Automatic inventories cleaning
# This allows you to schedule the purge of deleted inventoried
#
#
# TTL defines for how long an inventory stays in "Deleted Inventories" once deleted
# before it is purged (in days). There is no functional value of keeping these deleted inventories
# A negative value disable the automatic purging of deleted inventories.
#
# Check is made every interval hours
#
# Defaults: purge after being deleted for 7 days, check runs every 24 hours.
#

# TTL in days
rudder.batch.purge.inventories.delete.TTL=7
# Interval in hours
rudder.batch.purge.inventories.delete.interval=24

#
# Node information are put in a cache, and that cache is looked for change with following
# period. If a change is detected, a generation starts. Typically, it's what happen when a
# node inventory makes a change in hostname, ip, etc.
# Format is 'int timeunit', with time unit among: 's' (seconds), 'm' (minutes), 'h' (hours), 'd' (days).
#
rudder.batch.check.node.cache.interval=15 s

####################
# Webdav properties #################################################################
####################

#
# Authentication information for the webdav server used to
# receive Inventory Reports from nodes
#
rudder.webdav.user=rudder
rudder.webdav.password=rudder

####################################
# CFEngine and policies properties ##################################################
####################################

#
# Port used to distribute policies with cfengine cf-serverd, ie the port on
# which unix-based rudder agent contact the server to update their policies.
#
rudder.policy.distribution.port.cfengine=5309

#
# Port used to distribute policies by HTTPS, ie the port on which
# window-based rudder agent contact the server to update their policies.
#
rudder.policy.distribution.port.https=443

#
# Directories used to write nodes policies.
# - policies generated for a node with id UUID go in ***/var/rudder/share/UUID***
#   For now, that property can not be modified, because there is no simple
#   way of doing that and letting node knowing where to go look for their policies.
# - policies generated for Rudder Root Server go into ***/var/rudder/cfengine-community/inputs***

#
# Group owner for generated policy files. Default: rudder-policy-reader
#
rudder.generated.policies.group.owner=rudder-policy-reader

# - 'rudder.dir.backup' is the directory path where previous configuration of each node are stored
# CAUTION: For performance and consistency, it is necessary that the rudder.dir.backup is on the same
# filesystem as /var/rudder/share/ , otherwise the policies change for nodes is non-atomic (move becomes
# copy then delete), and can result to incomplete/partial policies being distributed to nodes.
# Comment that option to disable policy backup.
rudder.dir.backup=/var/rudder/share/backup/

#
# Shared folder
#
# Directory of the extra files the rudder root server will serve to the managed nodes
# If left empty, no extra files will be served
#
rudder.dir.shared.files.folder=/var/rudder/configuration-repository/shared-files

#
# Debug Node Configuration parameters
#
# Node Configurations are all the parameters (global parameters,
# applied rules/directives with their parameters, node information, etc)
# contextualized for the node. They are the resources used to actually
# generate policies for THAT node.
#
# By default, these information are only used internally by Rudder and
# are not available to the user. But they may be needed in some cases,
# for debugging.
#
# This option allows to define the directory where the node configurations
# (in JSON format) will go.
#
# To enable the writing of node configurations, enable the logger
# named "rudder.debug.nodeconfiguration" in logback.xml
#
rudder.debug.nodeconfiguration.path=/var/log/rudder/nodeConfigurations

####################
# Technique library #################################################################
####################

#
# The directory containing tools used by Rudder or nodes.
# You should configure the path of that directory to be
# the "tools" subdirectory of "rudder-technique" local
# clone of git repository
# (see property rudder.git.rudder-technique)
#
rudder.dir.dependencies=/var/rudder/tools

#
# Interval of time (in minutes) between two checks
# for a Technique library update.
#
# If O is given, the periodic update of Technique
# library features will be disabled
#
rudder.batch.techniqueLibrary.updateInterval=5

######################################################################
# Configuration and fact repositories, its update and Git properties ###########################
#####################################################################

#
# Configuration repository is the place where all Group/Directive/Rules
# configured by the user are historized.
#

#
# The full path to the directory containing the
# .git,  directives, groups and rules directories.
#
rudder.dir.gitRoot=/var/rudder/configuration-repository

#
# Group owner for files created by rudder application in configuration-repository
# directory (typically, configuration object serialization)
#
rudder.config.repo.new.file.group.owner=rudder

#
# Periodically do a Git GC to avoid accumulation of garbage.
# The schedule is defined based on a cron-like format with the usual fields plus a second one:
# seconds minutes hourOfDay dayOfTheMonth month DayOfTheWeek
# See https://www.alonsodomin.me/cron4s/userguide/#parsing for more information, and
# https://en.wikipedia.org/wiki/Cron for even more information.
#
# You can disable it totally with:
#   @never
# in place of the cron usual format.
# Even on big systems, once a day should be enough.
# Default run at 3:42 every morning.
#
rudder.git.gc=0 42 3 * * ?


#
# The full path to the directory containing the
# .git for facts: nodes
#
rudder.dir.gitRootFactRepo=/var/rudder/fact-repository

#
# By default, Rudder won"t save anything about node on file
# system. You can enable writing a JSON serialization of the
# in node fact directory (/var/rudder/fact-repository/nodes) with
# the following property set to true:
rudder.facts.repo.writeNodeState=false

# In addition to previous property, you can enable historization of
# all node change in the fact-repository git by setting the following
# property to true:
rudder.facts.repo.historizeNodeChange=false

###############################
# Dynamic group configuration  ######################################################
###############################

#
# Interval of time between two dynamic group update batch
# Expect an int (amount of minutes)
# If O is given, the dynamic group features will be disabled
#
rudder.batch.dyngroup.updateInterval=5

####################
# Inventory history ##################################################################
####################

#
# Inventory historization root directory
#
# The directory used as root directory to store LDIF dump
# of historized inventories.
# It must be synchronise with the property of the same name in
# the "inventory endpoint" web application (inventory-web.properties),
# which is the application actually writing the files.
#
# This historized inventories are used in the node acceptation
# screen, to keep information about the state of the node
# when it was accepted
#
history.inventories.rootdir=/var/rudder/inventories/historical

#
# Rudder used to store deleted node inventories in a dedicated LDAP branch:
# ou=Nodes,ou=Removed Inventories,ou=Inventories,cn=rudder-configuration
# As of Rudder 7.2, this feature is deprecated and node inventories are totally erased
# when a node is deleted, either by API or UI.
# Until Rudder 8.0, you can override that behaviour:
# - locally for API deletion with the corresponding parameter, see
#   https://docs.rudder.io/api/v/15/#tag/Nodes/operation/deleteNode
# - globally by changing the parameter of the following property from `erase` to `move`
rudder.nodes.delete.defaultMode = erase

#
# Periodically delete inventories under `/var/rudder/inventories/{received, failed}` directories.
# File in these two directories are purged independently from files under {incoming, accepted-nodes-updates}
# which are purged by the new inventory processor.  There is cron to configure when to delete them, and
# a max age in duration format.
#
# You can disable it totally with:
#   @never
# in place of the cron usual format.
#
rudder.inventories.cleanup.old.files.cron=0 32 3 * * ?
rudder.inventories.cleanup.old.files.age=30 days

###############################
# Non compliant reports logger #################################################
###############################

# Rudder can log a line for each 5 minute period when configuration policy is
# not correctly applied (in error or repaired).
#
# Default path is /var/log/rudder/compliance/non-compliant-reports.log, and can
# be changed in /opt/rudder/etc/logback.xml.
#
# See online documentation for more details.
#
# This log is generated by a job that runs at a regular interval, by default
# every minute. You can specify this interval (in minutes) below.
# A negative or 0 value disables the job, and won't log any non-compliant reports.
#

rudder.batch.reports.logInterval=1

#########################
# Store Agent Run Times  ############################################################
#########################

# Maximum catchup on reports at start (default : 30 minutes)
#
# To avoid handling too much reports at one time, the "Store Agent Run Times" process
# will only take reports from catchup time from now
# This is mainly used on the first Run, when the process has to catch on old reports,
# and build execution history.
# Set by two fields, maxDays in days, and maxMinutes in minutes

rudder.batch.storeAgentRunTimes.maxDays=0
rudder.batch.storeAgentRunTimes.maxMinutes=30

# Maximum batch size of report handled (default: 5 minutes)
#
# To avoid handling too much reports at one time, the "Store Agent Run Times" process
# will work on chunk of maxBatchSize minutes.
# This is mainly used to avoid using too much memory at once, and ease database

rudder.batch.storeAgentRunTimes.maxBatchSize=5

# Delay before to launch of the reports executions actor (default : 5)
#
# This value determine the frequency of the reports executions actor.
# This needs to be run very often so you should keep it as low as possible.
# Having a low delay will means that agent executions are almost synchronized with now.
# This value is expressed in seconds


rudder.batch.storeAgentRunTimes.updateInterval=5



###################################
# Rudder Users and Authentication  ###############################################################
###################################

#
# User information and sessions are stored in Rudder PostgreSQL database.
# These information can expire or may need to be purged (typically for compliance to privacy norms).
# Information are cleaned-up based on a cron-like schedule with format:
# seconds minutes hourOfDay dayOfTheMonth month DayOfTheWeek
# See https://www.alonsodomin.me/cron4s/userguide/#parsing for more information, and
# https://en.wikipedia.org/wiki/Cron for even more information.
#
# You can disable it totally with:
#   @never
# in place of the cron usual format.
# Default run at 1:17 every morning.
#
rudder.users.cleanup.cron=0 17 1 * * ?

# Other clean-up format are duration. You can use human format with unit: 'ms' 'milliseconds' up to 'days'
# For users:
# - when an user didn't logged-in for a certain period of time, its account can be disabled or/and
#   deleted. If the user never logged-in, then the account create date is used.
rudder.users.cleanup.account.disableAfterLastLogin=60 days
rudder.users.cleanup.account.deleteAfterLastLogin=120 days

# - when an user is deleted (for example removed from `rudder-user.xml` file), then the user information,
#   like status changes, are not purged immediately (typically to be able to do post-deletion accountability).
#   After some time, you will likely need to purge these information nonetheless.
#   If that parameter is set to '0' then users are purged immediately on deletion. We do not advice to do that.
rudder.users.cleanup.purgeDeletedAfter=30 days

# For sessions:
# - user session information (log-in time, permissions for the sessions, log-out, etc) need to be purge after
#   some time, too.
rudder.users.cleanup.sessions.purgeAfter=30 days


#
# Rudder has a root admin account, with full rights on the
# application, and whose authentication is independent from
# the authentication provider chosen (file, LDAP, etc).
# By default, the account is disabled (either by letting the
# the login or the password empty, or by commenting it).
#
# The password is a bcrypt hash, you can create it for example with the following command:
#  htpasswd -nBC 12 "" | tr -d ':\n'
# Where htpasswd is provided by apache tools.
#

#rudder.auth.admin.login=rootadmin
#rudder.auth.admin.password=secret

#
# Both authentication and authorization are handle in the rudder-users.xml
# file. You can use plugins to add other authentication backends to connect to
# your existing entreprise Active Directory or LDAP directory.
#
rudder.auth.provider=file

#
# Inactivity timeout for user sessions
#
# The default value is 30 minutes.
# Set an empty value to disable session expiration when the page stays open.
#
rudder.auth.idle-timeout=30 minutes

####################
# Server side Hooks #############################################################
####################

# This property contains the comma separated list of suffixes that will be checked
# before running a hook under /opt/rudder/etc/hooks.d.
# If an executable (+x) file has one of the following suffixes, it
# will be IGNORED and the corresponding hook skipped.
# Non executable (-x) files are ALWAYS ignored, with or without any of these suffixes.
#
# Spaces are trimmed. Case is not relevant (both .disabled and .DISABLED will be ignored)

rudder.hooks.ignore-suffixes= .swp, ~, .bak, \
 .cfnew   , .cfsaved  , .cfedited, .cfdisabled, .cfmoved,\
 .dpkg-old, .dpkg-dist, .dpkg-new, .dpkg-tmp,\
 .disable , .disabled , _disable , _disabled,\
 .sample  , .example  , .demo,\
 .ucf-old , .ucf-dist , .ucf-new ,\
 .rpmnew  , .rpmsave  , .rpmorig

########################
# Relayd reload command ########################################################
########################

# We have to way to identify nodes: by key of by certificated. When certificates,
# we update a file used by relayd at: /var/rudder/lib/ssl/allnodescerts.pem
# After change, we need to notify relayd to reload that file. This is the reload
# command.
#
rudder.relayd.reload=/opt/rudder/bin/rudder relay reload -p

###################
# Fatal exceptions #############################################################
###################

#
# This is a list of unhandled exception that should cause rudder to stop.
# When they happen, something went clearly wrong and even if rudder continues
# to work, it is most likely in an inconsistent state, so people should know
# it and not discover it at random when something else start go crazy.
# Subclasses of java.lang.Error always lead to termination.
#
rudder.jvm.fatal.exceptions=


########################
# DEPRECATED properties #############################################################
########################

#
# If true, an archive of Rules, groups,
# Directives and Active Techniques are recorded
# to the rudder.dir.gitRoot directory specified above
# and a git commit is performed when any of these items is modified.
# Boolean, defaults to true.
# You should change that value to "false" *only* if you do replication
# between rudder instances based on git. Rudder will always assume that
# directive/rules/parameters/etc are available in a serialized form at that
# place.
#
rudder.autoArchiveItems=true
